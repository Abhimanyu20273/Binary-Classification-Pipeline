# Binary-Classification-Pipeline

Binary Pipeline Documentation

Programs/Modules information : 

1) Train test split - Program to split the data into test and train set. It uses stratified train test split by default. To account for the imbalanced dataset stratified train test was performed. This ensured that the ratio of classes in both the test and train data remained the same.

2) T_test_Rank.py - The function of this module is to perform independent pairwise t-test and remove features on the basis of p-value threshold. The default p-value threshold used to remove features is 0.05  i.e. features which have greater than 0.05 p-value are removed. 

3) Variance_threshold.py - This module can be used to remove features on the basis of variance. The features which have variance lower than or equal to a particular threshold are removed. The default value for this threshold is 0.

4) Normalise.py - Program to  normalize data. It uses the standard scaler as default and also gives the option of using min max scaler
Standard Scaler uses the below mentioned formula to normalize the data:
z = (x - u) / s
Where u is the mean of the training data and s is the standard deviation of the training data
The min max scaler uses the following transformation:

X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
X_scaled = X_std * (max - min) + min

5) Proabability_generator.py : 
This module is used to replace the feature value of each sample by the probability of that sample belonging to the positive class based on that feature alone. For this purpose one of the following machine learning models is used : Logistic regression(default), Support Vector Machine, Elastic Net, Gaussian Naive Bayes and Random forest. The model is fitted on each individual feature and then the probability of each sample belonging to the positive class is determined using the model. These probabilities generated now can be used instead of the original feature values.
6) Individual_gene.py : 
This module finds the best individual features. It uses 5 fold cross validation to determine the best individual features. It takes the probability data generated by the previous module as input. In the probability data, the value of each feature of a sample indicates the probability of that sample belonging to the positive class. In this module a threshold for the probability is used to determine whether the sample belongs to the positive class or negative class. If the probability value is less than that threshold then the sample belongs to the negative class and if it is greater than a particular threshold then that sample belongs to the positive class. The threshold is varied from 0 to 1 in steps of 0.01. The threshold at which the difference between the sensitivity and specificity is minimum is taken to be the optimal threshold. The metrics calculated for each model are : AUROC, accuracy, precision, sensitivity, specificity, F1 score and MCC.The features are ranked on the basis of the best AUROC obtained on the test data. 


7) Feature removal on the basis of correlation : This module calculates the pairwise correlation for all the features. It removes one of the two features whose correlation is greater than a particular threshold. The default value of this threshold is 0.80. This module is embedded in the 

8) Combined_model_new.py : 
Finding best set of combined features: This modules finds the best set of combined features in 2 ways: 
Sequential feature selection : In this the top 100 features are taken on the basis of their individual performance as calculated by the best individual features module. First, the best combination of two features is found and that necessarily includes the best feature found. Thereafter the best combination of three features is found and that includes the two best features found in the previous step. This goes on till we have the best combination of a particular number of features as defined by the user(default is 20). Optimal threshold is chosen in this module as well and the principle used for that is the same as used in best individual features. The features are finally ranked on the basis of the best AUROC obtained. 
Combined probability : In this the probability data generated by convert to probability module is used. In the probability data for each sample the value of the feature is the probability for that sample belonging to the positive class based on that feature alone. When a combination of features is to be taken the probability values of each feature are added and the sum is divided by the number of features. Thus a value between 0 and 1 is again obtained which can be treated as a probability only. The number of features which are taken varies between 1 and the total number of features. The combination of features which are chosen are determined by the individual performance of the features. If a combination of n features is to be used then the best n individually performing features would be used

Pipeline usage guidelines : 
Module 1 : Train-test split
Command line arguments : 
1 ) "Inp_csv" : "The csv file which has the data with features”.
2 ) "label_column" : “Column name which has the labels. Only two labels are allowed."
3 ) "Stratify" : Default value is yes. Whether to use stratified train test split. By default program uses stratified train test split, enter no to use non-stratified train test split"

Files generated: 
1) Train_data.csv - Train data file
2) Test_data.csv - Test data file

Module 2 : T_test_Rank.py
Command line arguments : 
1 ) "Inp_csv" : "The csv file which has the data with features”.
2 ) "label_column" : “Column name which has the labels. Only two labels are allowed."
3 ) “Threshold’ : default value is 0.Threshold for feature removal based on p-value"
Files generated: 
1) Inp_csv : The csv file which has the data with features with features removed on the basis of p-value.
2) T_Test.csv - File containing the features ranked on the basis of the 

Module 3 : Variance_threshold.py
Command line arguments : 
1) “Train_csv” :  description= "The train data file"
2) "Test_csv" : description = "The test data file"
3)  "label_column" : “Column name which has the labels. Only two labels are allowed."
4) “threshold ": default = 0 , description = "Threshold for variance removal"
Files generated: 
1) Train_data.csv - Train data file with same name as original train data file
2) Test_data.csv - Test data file with same name as original test data file
Module 4 : Normalise.py
Command line arguments : 
1 ) “Train_csv” : The train data file.
2)”Test_csv" : "The test data file".
3) "label_column" : "Column name which has the labels"
4 ) "Scaler_type", default = "ss", description="Enter ss for standard scaler and mms for min max scaler"

Files generated: 
1) Scaled_train_data.csv - Scaled train data file. 
2) Scaled_test_data.csv - Scaled test data file.

Module 5 : proabability_generator.py
Command line arguments : 
1 ) "Train_csv" : default = "Scaled_train_data.csv"
description="The train data file"
2) “Test_csv" : default = "Scaled_test_data.csv"
description="The test data file".
3) "label_column" : description="Column name which has the labels"
4) “Model" : default = "LR", description = Model to be used. Enter LR for Logistic Regression,EN for elasticnet,NB for gaussian naive bayes,SVM for support vector machine and RF for random forest
5) parameter_optimization", default = "yes"
Description = "Enter yes for parameter optimization using grid search else enter no"
Files generated: 
1) Train_data_probability.csv - Train data file with the probability values
2) Test_data_probability.csv- Test data file with the probability values

Module 6 : Individual_gene.py
Command line arguments : 
1)  "train_data_csv", Description = "The training data file"
2) “label_column", Description = "Column name which has the labels"
3) "output_file", help="Name of the output file in which individual features would be ranked"
Files generated: 
1)  "Output_file":  File in which individual features would be ranked according to validation AUROC"
Module 7 : Correlation.py
Command line arguments : 
1) "Scaled_train_data_csv" : "The scaled train data file"
2) "Scaled_test_data_csv" :  "The scaled test data file"
3) "individual_gene_file" : "Individual feature file result"
4) "train_data_correlation" : "train data file which is unscaled"
5) "label_column" :Column name which has the labels"
“Threshold_” : default = 0.8, help="Threshold for removal of features based on correlation. By default it's 0.8.
Files generated: 
1) “Train_csv” - Train data file with same name as original train data file
2) "Test_csv" - Test data file with same name as original test data file

Module 8 : Combined_model_new.py
Command line arguments : 
1) “Train_data_csv" : The training data file
2) "Test_data_csv" : "The test data file"
3) "individual_feature_file" : File in which individual features are ranked according to their performance. 
4) "train_data_correlation" : “train_data_unscaled"
5) "remove_features_correlation" : Description="Enter 'Y' to remove features on the basis of correlation. Any other character otherwise. 
6) "label_column" : Description="Column name which has the labels"
7) "Model",default = "LR", Description="Model to be used. Enter LR for Logistic Regression,EN for elasticnet,NB for gaussian naive bayes,SVM for support vector machine and RF for random forest"
8) "num_top_features" : default = "20", Description="Number of top features to be used")
9) "output_file", Description="Name of output file"
Files generated: 
1) ”Train” +  “output_file” - Combined feature result on train data. Sorted according to the best AUROC. 
2) ”Validation” +  “output_file” - Use the best number of features found using the training dataset and use those numbers of features to generate the file for the metric on the test data. 







